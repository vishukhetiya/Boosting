{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "400a5392-1200-4f16-9dc4-f9c25f739593",
      "metadata": {
        "id": "400a5392-1200-4f16-9dc4-f9c25f739593"
      },
      "source": [
        "### Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fbf7530-1d04-4b9a-998a-58bebebd1d16",
      "metadata": {
        "id": "1fbf7530-1d04-4b9a-998a-58bebebd1d16"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**1. What is Boosting?**\n",
        "\n",
        "Boosting is an **ensemble learning technique** in machine learning. It combines **multiple weak learners** (usually decision trees) to form a **strong learner** with high accuracy.\n",
        "\n",
        "- A weak learner is a model that performs slightly better than random guessing.\n",
        "- Boosting trains these weak models **sequentially**, one after the other.\n",
        "\n",
        "---\n",
        "\n",
        "**2. How Boosting Works:**\n",
        "\n",
        "1. Start by training the first model on the original data.\n",
        "2. Check which samples were predicted incorrectly.\n",
        "3. Increase the importance (weight) of the misclassified samples.\n",
        "4. Train the next model focusing more on these hard samples.\n",
        "5. Repeat the process for a fixed number of rounds.\n",
        "6. Combine all the weak learners using a weighted sum of their predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Example - AdaBoost (Adaptive Boosting):**\n",
        "\n",
        "- In AdaBoost, each weak model is assigned a weight based on how well it performs.\n",
        "- Misclassified samples get more weight so that the next model focuses on them.\n",
        "- Final prediction is based on a **weighted vote** of all weak learners.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Why Boosting Improves Weak Learners:**\n",
        "\n",
        "- Boosting makes each new model correct the **mistakes** of the previous one.\n",
        "- This helps in reducing **bias** and improving **accuracy**.\n",
        "- By focusing on the errors, it creates a model that learns from its weaknesses.\n",
        "- Even if each learner is weak, their combination becomes strong.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Popular Boosting Algorithms:**\n",
        "\n",
        "- **AdaBoost** (Adaptive Boosting)\n",
        "- **Gradient Boosting**\n",
        "- **XGBoost** (Extreme Gradient Boosting)\n",
        "- **LightGBM**\n",
        "- **CatBoost**\n",
        "\n",
        "---\n",
        "\n",
        "**6. Summary:**\n",
        "\n",
        "- Boosting is a method that builds models sequentially.\n",
        "- Each model tries to fix the errors of the previous model.\n",
        "- It turns weak learners into a powerful predictive model.\n",
        "- It is commonly used in real-world problems due to its high performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6af7ab66-230b-4e5d-98ef-a08810b8463b",
      "metadata": {
        "id": "6af7ab66-230b-4e5d-98ef-a08810b8463b"
      },
      "source": [
        "### Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2558c58b-5d02-4e37-b0ad-5267a8f7415d",
      "metadata": {
        "id": "2558c58b-5d02-4e37-b0ad-5267a8f7415d"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Both AdaBoost and Gradient Boosting are **boosting algorithms** that build a strong model by combining multiple weak learners (usually decision trees). However, they follow different strategies for training models. Let's understand them step-by-step.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Core Idea\n",
        "- **AdaBoost**: Focuses on **adjusting weights** of training samples based on the performance of previous models.\n",
        "- **Gradient Boosting**: Focuses on **minimizing errors** using **gradient descent** on a loss function.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Training Process\n",
        "\n",
        "**AdaBoost:**\n",
        "- All samples start with equal weight.\n",
        "- After each model is trained:\n",
        "  - Increase the weight of **misclassified samples**.\n",
        "  - Decrease the weight of correctly predicted samples.\n",
        "- The next model focuses more on the samples with higher weight.\n",
        "- Final prediction is a **weighted vote** of all models.\n",
        "\n",
        "**Gradient Boosting:**\n",
        "- Starts with a basic model (e.g., predicting the mean).\n",
        "- Calculates the **residual errors** (difference between actual and predicted values).\n",
        "- Fits the next model on these residuals.\n",
        "- Each new model tries to **correct the errors** of the previous model.\n",
        "- Final prediction is the **sum of all models**' predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Error Handling\n",
        "- **AdaBoost**: Learns from errors by **reweighting samples**.\n",
        "- **Gradient Boosting**: Learns from errors by **modeling residuals**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Loss Function\n",
        "- **AdaBoost**: Uses **exponential loss function** (in original form).\n",
        "- **Gradient Boosting**: Can use **any differentiable loss function** (e.g., mean squared error, log loss).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Flexibility\n",
        "- **AdaBoost**: Less flexible in terms of choosing loss functions.\n",
        "- **Gradient Boosting**: More flexible, allows customization of loss function.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Summary Table\n",
        "\n",
        "| Feature                 | AdaBoost                                  | Gradient Boosting                         |\n",
        "|-------------------------|-------------------------------------------|-------------------------------------------|\n",
        "| Focus                  | Sample weights                             | Residual errors                           |\n",
        "| Error Handling         | Increase weight on misclassified samples   | Fit new model to residuals                |\n",
        "| Loss Function          | Exponential loss                           | Any differentiable loss                   |\n",
        "| Flexibility            | Limited                                    | High flexibility                          |\n",
        "| Model Combination      | Weighted majority vote                     | Additive model (sum of predictions)       |\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Conclusion\n",
        "- AdaBoost and Gradient Boosting both improve model performance by combining weak learners.\n",
        "- AdaBoost reweights data points to focus on hard cases.\n",
        "- Gradient Boosting uses gradients to reduce prediction errors.\n",
        "- Gradient Boosting is more flexible and widely used in real-world problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0162680a-817f-42ca-80ce-b649d14cbb44",
      "metadata": {
        "id": "0162680a-817f-42ca-80ce-b649d14cbb44"
      },
      "source": [
        "### Question 3: How does regularization help in XGBoost?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dddea55-b3ce-4c2e-917f-8d04689830d1",
      "metadata": {
        "id": "9dddea55-b3ce-4c2e-917f-8d04689830d1"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "### 1. What is Regularization?\n",
        "- Regularization is a technique used in machine learning to **prevent overfitting**.\n",
        "- It adds a **penalty** to the model's complexity.\n",
        "- The idea is to make the model simpler and more general.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. What is XGBoost?\n",
        "- XGBoost stands for **Extreme Gradient Boosting**.\n",
        "- It is a powerful and efficient implementation of gradient boosting.\n",
        "- XGBoost includes regularization as part of its algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Regularization in XGBoost\n",
        "- XGBoost includes **L1** and **L2 regularization** in its objective function.\n",
        "- These are similar to what we use in **Lasso (L1)** and **Ridge (L2)** regression.\n",
        "\n",
        "**Objective Function in XGBoost:**\n",
        "\n",
        "```\n",
        "Obj = Loss + Ω(f)\n",
        "Where:\n",
        "Loss = Training loss (how well the model fits the data)\n",
        "Ω(f) = Regularization term = γT + 0.5 * λ * Σ(w_j)^2\n",
        "```\n",
        "- **γ (gamma)**: Penalty for adding more leaves to the tree.\n",
        "- **λ (lambda)**: L2 regularization on leaf weights (makes them smaller).\n",
        "- **α (alpha)**: L1 regularization (makes some weights exactly zero).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. How Regularization Helps:\n",
        "\n",
        "**a. Prevents Overfitting:**\n",
        "- Penalizes complex trees with too many leaves.\n",
        "- Keeps leaf weights small and avoids large jumps in predictions.\n",
        "\n",
        "**b. Improves Generalization:**\n",
        "- Model performs better on unseen data.\n",
        "- Reduces variance.\n",
        "\n",
        "**c. Controls Tree Complexity:**\n",
        "- γ prevents too many splits.\n",
        "- λ and α shrink the leaf values.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. When to Use Regularization\n",
        "- If your model is performing very well on training data but poorly on test data, it's overfitting.\n",
        "- In such cases, increasing regularization parameters can help.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Summary\n",
        "- Regularization in XGBoost is used to **control model complexity**.\n",
        "- It adds penalties to discourage overly complex trees.\n",
        "- This results in better generalization and **more robust models**.\n",
        "- The key regularization parameters are **gamma, lambda, and alpha**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df5d6e8-c570-4788-b218-d04aeac1518b",
      "metadata": {
        "id": "9df5d6e8-c570-4788-b218-d04aeac1518b"
      },
      "source": [
        "### Question 4: Why is CatBoost considered efficient for handling categorical data?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91bbfd27-bf83-42bb-93dc-2850ff191d8f",
      "metadata": {
        "id": "91bbfd27-bf83-42bb-93dc-2850ff191d8f"
      },
      "source": [
        "**Answer:**\n",
        "CatBoost is a machine learning algorithm developed by Yandex. It belongs to the family of gradient boosting algorithms and is especially designed to efficiently handle **categorical features**, which are very common in real-world data.\n",
        "\n",
        "## Key Reasons Why CatBoost is Efficient for Categorical Data:\n",
        "\n",
        "### 1. **Native Categorical Feature Support**\n",
        "CatBoost can directly take categorical features as input. Unlike other algorithms (like XGBoost or LightGBM) which require you to convert categorical features using one-hot encoding or label encoding, CatBoost handles this internally.\n",
        "\n",
        "### 2. **Efficient Encoding Using Target Statistics**\n",
        "CatBoost uses a technique called **Ordered Target Statistics**. Instead of computing average target values for each category using all data (which causes target leakage), it computes them in a way that avoids using the current row's label. This makes training more accurate and prevents overfitting.\n",
        "\n",
        "### 3. **Less Preprocessing Required**\n",
        "You don't have to manually encode categorical columns before passing them to CatBoost. This reduces preprocessing steps and chances of human error.\n",
        "\n",
        "### 4. **Faster Training with Categorical Splits**\n",
        "CatBoost creates efficient splits on categorical features during tree building. It avoids brute-force search by using smart methods for finding useful splits.\n",
        "\n",
        "### 5. **Built-In Support for High Cardinality Features**\n",
        "Even if a categorical feature has hundreds or thousands of unique values (high cardinality), CatBoost handles it well. This is important in datasets with variables like 'user_id', 'product_code', etc.\n",
        "\n",
        "### 6. **Better Accuracy on Categorical Datasets**\n",
        "Since the encoding method is more principled and avoids leakage, CatBoost often gives better accuracy than models trained with manually encoded features.\n",
        "\n",
        "## Summary\n",
        "CatBoost is efficient for categorical data because it automates encoding, avoids data leakage, supports high-cardinality features, and reduces preprocessing. It simplifies the model development process and improves model accuracy on datasets with categorical variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "885ea446-73e4-4c75-85cf-309ed8f855bb",
      "metadata": {
        "id": "885ea446-73e4-4c75-85cf-309ed8f855bb"
      },
      "source": [
        "### Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74348ba5-b70e-4833-815e-957becf2f3be",
      "metadata": {
        "id": "74348ba5-b70e-4833-815e-957becf2f3be"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Boosting and Bagging are both popular ensemble learning techniques, but they are used in different scenarios based on the problem characteristics. Boosting techniques such as AdaBoost, Gradient Boosting, and XGBoost are generally preferred in situations where **high prediction accuracy is critical** and the model can benefit from reducing bias through sequential learning.\n",
        "\n",
        "## Real-World Applications Where Boosting is Preferred\n",
        "\n",
        "### 1. **Credit Scoring and Risk Modeling in Finance**\n",
        "- **Use Case**: Predicting loan default, credit risk assessment, and fraud detection.\n",
        "- **Why Boosting**: Boosting can handle complex patterns in imbalanced data and improves performance by focusing on misclassified cases (e.g., high-risk customers).\n",
        "\n",
        "### 2. **Customer Churn Prediction**\n",
        "- **Use Case**: Identifying which customers are likely to stop using a service.\n",
        "- **Why Boosting**: These datasets often have class imbalance. Boosting improves precision and recall, making it suitable for identifying rare churn cases.\n",
        "\n",
        "### 3. **Click-Through Rate (CTR) Prediction in Online Advertising**\n",
        "- **Use Case**: Predicting if a user will click on an ad.\n",
        "- **Why Boosting**: CTR data is sparse and has high cardinality. Boosting algorithms like XGBoost and LightGBM are widely used because they offer better accuracy and efficiency.\n",
        "\n",
        "### 4. **Medical Diagnosis and Prognosis**\n",
        "- **Use Case**: Predicting diseases such as cancer based on medical records.\n",
        "- **Why Boosting**: Boosting provides more accurate models by reducing bias. This is important in healthcare where decision errors can be costly.\n",
        "\n",
        "### 5. **Image Classification and Object Detection**\n",
        "- **Use Case**: Classifying images or detecting objects in security cameras.\n",
        "- **Why Boosting**: Some boosting algorithms like AdaBoost with Haar features are used in face detection systems.\n",
        "\n",
        "### 6. **Fraud Detection in Banking and E-commerce**\n",
        "- **Use Case**: Detecting fraudulent credit card transactions or e-commerce purchases.\n",
        "- **Why Boosting**: Boosting helps in focusing on difficult-to-classify examples, which is common in fraud cases.\n",
        "\n",
        "## Comparison with Bagging Methods\n",
        "\n",
        "- **Bagging (e.g., Random Forest)** is generally preferred when the primary concern is reducing variance and when the data is very noisy.\n",
        "- **Boosting** is preferred when we need to reduce both bias and variance, and when we want higher accuracy for business-critical tasks.\n",
        "\n",
        "## Summary\n",
        "Boosting is commonly used in high-stakes applications such as finance, healthcare, advertising, and fraud detection where reducing bias and improving prediction accuracy is crucial. It is especially powerful when data is imbalanced, complex, or requires sequential learning to correct mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JJM9uX_5WQFG",
      "metadata": {
        "id": "JJM9uX_5WQFG"
      },
      "source": [
        "Datasets:\n",
        "\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "026365de-c9cb-44b5-86f9-f2e10fcb29eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "026365de-c9cb-44b5-86f9-f2e10fcb29eb",
        "outputId": "be993187-dccb-46e3-c7c8-9f294ba86b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9649122807017544\n"
          ]
        }
      ],
      "source": [
        "### **Answer:**\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target labels\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the AdaBoost classifier\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y-AQY06gWVf8",
      "metadata": {
        "id": "Y-AQY06gWVf8"
      },
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246f9f67-8020-49fa-8140-a97a96e9bbdc",
      "metadata": {
        "id": "246f9f67-8020-49fa-8140-a97a96e9bbdc"
      },
      "source": [
        "**Answer:**\n",
        "This notebook demonstrates how to:\n",
        "- Load the California Housing dataset\n",
        "- Train a Gradient Boosting Regressor\n",
        "- Evaluate its performance using R-squared score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5975ad5f-1296-4b20-9deb-ff79004a49a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5975ad5f-1296-4b20-9deb-ff79004a49a7",
        "outputId": "a8224226-28ec-4325-f8ab-ffd64347d83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='Target')\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate using R-squared score\n",
        "y_pred = gbr.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared Score:\", round(r2, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3gXIXSX-WZu5",
      "metadata": {
        "id": "3gXIXSX-WZu5"
      },
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bCLe1MvwWeoN",
      "metadata": {
        "id": "bCLe1MvwWeoN"
      },
      "source": [
        "Answer -\n",
        "In this task, we will:\n",
        "- Load the Breast Cancer dataset from `sklearn.datasets`\n",
        "- Train an XGBoost Classifier\n",
        "- Tune the learning rate using `GridSearchCV`\n",
        "- Print the best parameters and accuracy score\n",
        "\n",
        "Before running the code, make sure the `xgboost` library is installed. You can install it by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b191e3d9-a2c5-4b13-b91c-d71246d1120f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b191e3d9-a2c5-4b13-b91c-d71246d1120f",
        "outputId": "d97ce8a9-e43c-489d-9316-fd8018a9ba48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.1)\n"
          ]
        }
      ],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e5f2efc0-b9bb-405d-b8e1-47d9f68d4eb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5f2efc0-b9bb-405d-b8e1-47d9f68d4eb0",
        "outputId": "fb6a7487-4602-4546-e0fc-1ca0dc9c21cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Accuracy on Test Set: 0.956140350877193\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode the target labels\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "# Initialize XGBoost Classifier (without 'use_label_encoder')\n",
        "xgb = XGBClassifier(eval_metric='logloss')\n",
        "\n",
        "# Define the parameter grid for learning rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to tune the learning rate\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train_enc)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test_enc, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Accuracy on Test Set:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rk07qwZyWuss",
      "metadata": {
        "id": "Rk07qwZyWuss"
      },
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5c229011-62ec-42fa-8a9d-6788d29402cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c229011-62ec-42fa-8a9d-6788d29402cc",
        "outputId": "dea4a57f-c13a-40d4-ffd0-3446a6293624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cbedcce6-62ac-4097-b170-1bf702af036c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "cbedcce6-62ac-4097-b170-1bf702af036c",
        "outputId": "fa52c450-2386-4ba5-e5cb-c639ad4f1ba7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Separate features and target\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoostClassifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,        # Number of boosting iterations\n",
        "    learning_rate=0.1,     # Learning rate\n",
        "    depth=6,               # Depth of the tree\n",
        "    verbose=0              # Turn off training logs\n",
        ")\n",
        "\n",
        "# Train the model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Convert to DataFrame for better labels\n",
        "cm_df = pd.DataFrame(cm, index=['Actual: 0', 'Actual: 1'], columns=['Predicted: 0', 'Predicted: 1'])\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n2G0r0_cXg8R",
      "metadata": {
        "id": "n2G0r0_cXg8R"
      },
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WU663HxpXhxb",
      "metadata": {
        "id": "WU663HxpXhxb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "18797af2",
      "metadata": {
        "id": "18797af2"
      },
      "source": [
        "## Question 10: Loan Default Prediction with Boosting Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "74358203",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74358203",
        "outputId": "df018a5c-1669-49b0-f4a7-e0bd3cf9fa39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50}\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75         6\n",
            "           1       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.60        10\n",
            "   macro avg       0.30      0.50      0.38        10\n",
            "weighted avg       0.36      0.60      0.45        10\n",
            "\n",
            "Confusion Matrix:\n",
            " [[6 0]\n",
            " [4 0]]\n",
            "ROC AUC Score: 1.0\n",
            "\n",
            "Business Benefit:\n",
            "This model helps the FinTech company accurately identify high-risk customers, enabling proactive measures such as adjusting credit limits, offering financial counseling, or requiring additional documentation before loan approval. This reduces default rates, improves profitability, and enhances customer trust.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [15:26:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Question 10 Solution\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Example synthetic dataset creation (replace with actual dataset in production)\n",
        "data = pd.DataFrame({\n",
        "    'age': [25, 40, np.nan, 35, 50, 28, 42, np.nan, 33, 29],\n",
        "    'income': [50000, 60000, 55000, np.nan, 80000, 52000, 62000, 58000, np.nan, 54000],\n",
        "    'gender': ['M', 'F', 'F', 'M', np.nan, 'M', 'F', 'F', 'M', 'M'],\n",
        "    'transaction_count': [5, 15, 10, 7, 20, 6, 12, 9, 14, 8],\n",
        "    'default': [0, 1, 0, 0, 1, 0, 1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "# Split features and target\n",
        "X = data.drop('default', axis=1)\n",
        "y = data['default']\n",
        "\n",
        "# Identify column types\n",
        "num_features = ['age', 'income', 'transaction_count']\n",
        "cat_features = ['gender']\n",
        "\n",
        "# Preprocessing\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', num_transformer, num_features),\n",
        "    ('cat', cat_transformer, cat_features)\n",
        "])\n",
        "\n",
        "# XGBoost Classifier\n",
        "xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb)\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100],\n",
        "    'classifier__max_depth': [3, 5],\n",
        "    'classifier__learning_rate': [0.01, 0.1]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='roc_auc')\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = grid_search.predict(X)\n",
        "y_proba = grid_search.predict_proba(X)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Classification Report:\\n\", classification_report(y, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y, y_pred))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y, y_proba))\n",
        "\n",
        "# Business Benefit Explanation\n",
        "print(\"\\nBusiness Benefit:\")\n",
        "print(\"This model helps the FinTech company accurately identify high-risk customers, enabling proactive measures such as adjusting credit limits, offering financial counseling, or requiring additional documentation before loan approval. This reduces default rates, improves profitability, and enhances customer trust.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}